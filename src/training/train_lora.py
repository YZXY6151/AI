#!/usr/bin/env python3
import os
import json
import torch
from transformers.utils.logging import disable_progress_bar
disable_progress_bar()

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
from transformers import EarlyStoppingCallback, set_seed

# â”€â”€â”€ ç¯å¢ƒæ¸…ç†ï¼šå¼ºåˆ¶å•å¡è¿è¡Œ â”€â”€â”€
os.environ["CUDA_VISIBLE_DEVICES"]        = "0"
os.environ["LOCAL_RANK"]                 = "0"
os.environ["RANK"]                       = "0"
os.environ["WORLD_SIZE"]                 = "1"
os.environ["ACCELERATE_DISABLE_MAPPING"] = "true"
os.environ.setdefault("MASTER_ADDR", "127.0.0.1")
os.environ.setdefault("MASTER_PORT", "29500")

def main():
    # â”€â”€â”€ è·¯å¾„é…ç½® â”€â”€â”€
    model_name       = "models/yi-1.5-9b"
    lora_config_path = "configs/lora_config.json"
    train_file       = "data/finetune/train.jsonl"
    valid_file       = "data/finetune/valid.jsonl"
    output_dir       = "models/yi-lora"

    # â€”â€” å›ºå®šéšæœºç§å­ï¼Œä¿è¯å¯å¤ç° â€”â€” 
    set_seed(42)

    # â”€â”€â”€ Tokenizer & Base Model â”€â”€â”€
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
    # æŠŠ pad_token è®¾ç½®æˆ eosï¼Œä»¥ä¾¿åç»­ collator èƒ½æ­£ç¡® pad
    tokenizer.pad_token = tokenizer.eos_token

    sep = tokenizer.eos_token or "\n"

    bnb_config = BitsAndBytesConfig(load_in_8bit=True)
    base_model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map=None
    )
    base_model = prepare_model_for_kbit_training(base_model)

    print(f"ğŸš€ å½“å‰æ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

    # â”€â”€â”€ åŠ è½½ & æ³¨å…¥ LoRA â”€â”€â”€
    with open(lora_config_path) as f:
        lora_cfg = LoraConfig(**json.load(f))
    model = get_peft_model(base_model, lora_cfg)

    # â”€â”€â”€ Trainable å‚æ•°å æ¯” â”€â”€â”€
    total, trainable = 0, 0
    for _, p in model.named_parameters():
        total += p.numel()
        if p.requires_grad:
            trainable += p.numel()
    print(f"ğŸš€ Trainable params: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)")

    # â”€â”€â”€ åŠ è½½æ•°æ®é›† & Tokenizeï¼ˆåªè¾“å‡º input_ids, attention_maskï¼‰ â”€â”€â”€
    ds = load_dataset("json", data_files={"train": train_file, "validation": valid_file})

    def tokenize_fn(example):
        text = example["prompt"] + sep + example["completion"]
        return tokenizer(text, truncation=True, max_length=512)

    train_ds = ds["train"].map(
        tokenize_fn,
        batched=False,
        remove_columns=["prompt", "completion", "metadata"]
    )
    eval_ds = ds["validation"].map(
        tokenize_fn,
        batched=False,
        remove_columns=["prompt", "completion", "metadata"]
    )

    # â”€â”€â”€ ä½¿ç”¨ DataCollatorForLanguageModeling (mlm=False) â”€â”€â”€
    # è‡ªåŠ¨å¤åˆ¶ inputs åˆ° labels å¹¶åšåŠ¨æ€ padï¼Œä¿è¯ä¸‰è€…å¯¹é½ :contentReference[oaicite:0]{index=0}
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )

    # â”€â”€â”€ è®­ç»ƒå‚æ•° â”€â”€â”€
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=8,
        learning_rate=5e-5,               # ä½ LRï¼Œå¹³æ»‘æ”¶æ•›
        warmup_steps=300,                 # æ›´é•¿ warmup
        lr_scheduler_type="linear",       # çº¿æ€§è¡°å‡
        num_train_epochs=5,               # å¤šè·‘å‡ è½®ï¼Œç”¨ EarlyStopping å†³å®šä½•æ—¶åœ
        logging_steps=1,
        weight_decay=0.01,            # L2 æ­£åˆ™ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        max_grad_norm=1.0,            # æ¢¯åº¦è£å‰ªï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸
        dataloader_drop_last=True,        # ä¿è¯ batch å¤§å°ä¸€è‡´
        load_best_model_at_end=True,      # è®­ç»ƒç»“æŸåè‡ªåŠ¨åŠ è½½ â€œæœ€ä½³â€ checkpoint
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        save_strategy="steps",
        save_steps=500,                   # ç¨å¾®å‡å°‘ checkpoint é¢‘ç‡
        save_total_limit=3,  
        eval_strategy="steps",
        eval_steps=500,
        fp16=True,
        optim="paged_adamw_8bit",
        gradient_checkpointing=True,
        ddp_find_unused_parameters=False,
        report_to="none",
        label_names=["labels"],  # Trainer ç”¨æ¥è®¡ç®— loss :contentReference[oaicite:1]{index=1}
    )

    # â”€â”€â”€ Trainer â”€â”€â”€
    trainer = Trainer(
        model=model,
        eval_dataset=eval_ds, 
        args=training_args,
        train_dataset=train_ds,
        tokenizer=tokenizer,         # è®© Trainer è‡ªåŠ¨åº”ç”¨ tokenizer çš„ dynamic padding :contentReference[oaicite:2]{index=2}
        data_collator=data_collator,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
    )

    # â”€â”€â”€ å¯åŠ¨è®­ç»ƒ â”€â”€â”€
    trainer.train()
    trainer.save_model(output_dir)
    print("âœ… LoRA å¾®è°ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åœ¨", output_dir)


if __name__ == "__main__":
    main()
